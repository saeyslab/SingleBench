% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/02_Evaluate_.R
\name{Evaluate}
\alias{Evaluate}
\title{Evaluate a benchmark}
\usage{
Evaluate(
  benchmark,
  score_projections,
  projection_neighbourhood,
  n_cores,
  which_python,
  seed.projection,
  seed.clustering,
  ask_overwrite,
  verbose
)
}
\arguments{
\item{benchmark}{object of class \code{Benchmark}, as generated by the constructor \code{Benchmark}}

\item{score_projections}{logical: whether results of projection steps should be scored. Default value is \code{FALSE}}

\item{projection_neighbourhood}{integer: size of neighbourhood considered as 'local' for scoring of projections. Default value is \code{100}}

\item{n_cores}{optional integer: number of CPU threads to use for parallelisation of repeated runs of clustering for stability analysis. Default value is \code{NULL} (no parallelisation)}

\item{which_python}{optional string: path to Python if Python needs to be used via \code{reticulate}. Default value is \code{NULL} (\code{reticulate} uses its default Python configuration)}

\item{seed.projection}{optional numeric value: value random seed to be used prior to each deployment of a projection method. (Use \code{NULL} to avoid setting a seed.) Default value is \code{1}}

\item{seed.clustering}{optional numeric value: value random seed to be used prior to each deployment of a clustering method. (Use \code{NULL} to avoid setting a seed.) Default value is \code{1}}

\item{ask_overwrite}{logical: if \code{benchmark} was evaluated before, should the user be asked prior to overwriting the previous evaluation results? Default value is \code{TRUE}}

\item{verbose}{logical: should progress messages be printed during evaluation? Default value is \code{TRUE}}
}
\description{
This function evaluates a benchmark pipeline, specified by an object of type \code{Benchmark}.
This means that all the projection, clustering or projection->clustering subpipelines that were set up when creating the benchmark object are executed, and their performance is scored.
Both the benchmark object and its auxiliary HDF5 file (created when the \code{Benchmark} constructor was called) are needed for this.
}
\section{Optional scoring of projection steps}{
Optionally, results of projection steps (if included) can be scored using evaluation metrics designed to measure the quality of dimension reduction (preservation of information versus original high-dimensional data).
This makes sense for methods that reduce dimensionality of the original data for the purposes of visualisation or to make the data more amenable to clustering.
To turn on scoring of projection steps, set parameter \code{score_projection} to \code{TRUE}.
Beware, this is only tractable for small datasets, due to the necessity to compute a co-ranking matrix (quadratic complexity with size of dataset).
The local continuity meta-criterion (LCMC), trustworthiness and continuity are computed.
}

\section{Parallelisation}{
For stability analysis of clustering tools, repeated runs of the tool can be run in parallel (unless this is forbidden in the tool wrapper).
The \code{parallel}, \code{doParallel} and \code{foreach} packages are needed for this.
To do this, specify the parameter \code{n_cores}.
Use \code{parallel::detectCores()} to check how many CPU cores are available.
}

\seealso{
\itemize{
\item \strong{\code{AddLayout}}: allows you to add a separate 2-dimensional layout of the input dataset or to use an existing projection (produced in the evaluation) as a visualisation layout.
}
}
